# Project Brief: Siriusx-API

**Version:** 1.0
**Date:** 2025-09-30
**Status:** Draft
**Author:** Business Analyst Team

---

## Executive Summary

**Siriusx-API** 是一个面向个人开发者的轻量级 AI 模型聚合网关，旨在解决多供应商 AI 服务管理中的核心痛点：模型命名混乱、格式不兼容和单点故障风险。

通过统一的模型命名、灵活的格式转换（OpenAI / Claude）和智能负载均衡，Siriusx-API 将复杂的多供应商管理"隐藏"在优雅的接口之下，让开发者能够"无感"地享受高可用、低成本的 AI 基础设施。

**核心价值主张：** 把繁琐、脏的东西埋在下面，让用户无感知地使用可靠的 AI 服务。

**目标市场：** 个人开发者和小型团队（2-5人），特别是正在开发 AI 应用、使用多个供应商、需要轻量级部署的用户。

**部署方式：** Docker 容器化，开箱即用。

---

## Problem Statement

### 当前状态与痛点

个人开发者在使用多个 AI 服务供应商时，面临三大核心痛点：

#### 1. 模型命名混乱 😰

不同供应商对同一个模型使用不同的命名规则：
- 供应商 A：`claude-4`
- 供应商 B：`claude-4-sonnet`
- 供应商 C：`claude-sonnet-4`

**影响：**
- 配置管理混乱，难以维护
- 无法快速识别和切换相同模型
- 增加认知负担和出错概率

#### 2. 格式不兼容 🔧

不同工具和场景需要不同的 API 格式：
- 普通应用需要 OpenAI 兼容格式（`/v1/chat/completions`）
- Claude Code 需要 Claude 原生格式（`/v1/messages`）
- 现有解决方案（如 new-api）对 Claude Code 支持不佳，需要额外的 claudecoderouter 转发

**影响：**
- 工具锁定，无法灵活切换
- 需要为每个工具单独配置
- 增加系统复杂度和维护成本

#### 3. 单点故障风险 ⚠️

依赖单一供应商存在高可用性风险：
- 供应商限流或宕机导致服务中断
- 手动切换供应商流程繁琐，影响用户体验
- 缺乏自动故障转移机制

**影响：**
- 服务可靠性低
- 需要 7x24 人工监控
- 影响最终用户体验

### 现有解决方案的不足

**new-api：**
- ✅ 功能全面（渠道管理、用户系统、额度管理等）
- ❌ 模型命名统一困难（架构限制）
- ❌ Claude Code 支持麻烦（需要额外转发）
- ❌ 功能过于臃肿，不再轻量简洁
- ❌ 主要面向企业/团队，对个人开发者过于复杂

**其他网关（LiteLLM、One API 等）：**
- 普遍缺少细粒度的模型名称统一能力
- 格式转换支持有限
- 配置复杂度较高

### 为什么现在解决这个问题

1. **AI 应用爆发增长**：越来越多个人开发者在构建 AI 应用
2. **多供应商成为常态**：成本优化和可靠性需求驱动开发者使用多个供应商
3. **工具生态多样化**：ChatGPT Web、Claude Code、Cursor 等工具各有格式要求
4. **轻量化需求**：个人开发者需要简单、可靠、易部署的解决方案

---

## Proposed Solution

### 核心概念

Siriusx-API 是一个**极简主义**的 AI 模型聚合网关，专注于四个核心功能，但将每个功能做到极致：

```
┌──────────────────────────────────────────┐
│         Siriusx-API 核心架构             │
├──────────────────────────────────────────┤
│                                          │
│  🏢 1. 供应商管理                        │
│     ├─ URL + API Key 管理                │
│     ├─ 自动模型列表发现 (/v1/models)     │
│     └─ 健康状态监控                      │
│                                          │
│  🎯 2. 模型聚合                          │
│     ├─ 统一命名标准化                    │
│     ├─ 供应商-模型级映射                 │
│     └─ 权重 + 优先级配置                 │
│                                          │
│  🔄 3. 格式转换                          │
│     ├─ OpenAI 格式 (/v1/chat/...)        │
│     ├─ Claude 格式 (/v1/messages)        │
│     └─ 自定义端点配置                    │
│                                          │
│  ⚖️ 4. 负载均衡                          │
│     ├─ 默认权重平均分配                  │
│     ├─ 用户可手动调整                    │
│     └─ 自动故障检测与转移                │
│                                          │
└──────────────────────────────────────────┘
```

### 核心差异化特性

#### 1. 统一的模型命名聚合

**问题：** 供应商 A 的 `claude-4`、B 的 `claude-4-sonnet`、C 的 `claude-sonnet-4` 都是同一个模型

**Siriusx-API 方案：**
```
统一模型名: claude-sonnet-4
  ├─ 供应商 A 的 claude-4 (权重 50%, 优先级 1)
  ├─ 供应商 B 的 claude-4-sonnet (权重 30%, 优先级 2)
  └─ 供应商 C 的 claude-sonnet-4 (权重 20%, 优先级 3)
```

**价值：** 用户只需记住一个标准化的模型名称，系统自动管理底层的多个供应商。

#### 2. 灵活的双端点格式转换

**核心创新：** 同一个聚合模型，可以提供多种输出格式

**场景示例：**

```
场景 A：给 ChatGPT Next Web 用
模型名: claude-sonnet-4
端点: /v1/chat/completions (OpenAI 格式)
→ 无需改代码，直接使用

场景 B：给 Claude Code 用
模型名: cc-claude4 (自定义)
端点: /v1/messages (Claude 原生格式)
→ 获得更好的兼容性和完整特性
```

**技术实现：**
- 上游统一接收：基于 new-api 的 OpenAI 兼容格式
- 下游灵活输出：自动转换为目标格式
- 自定义端点：为不同工具创建专用模型端点

#### 3. 智能负载均衡与故障转移

**工作流程：**
```
请求 claude-sonnet-4
  ↓
根据权重分配到供应商 A (50%)
  ↓
供应商 A 限流/失败？
  ↓ 是
自动切换到供应商 B 或 C
  ↓
用户完全无感知 ✨
```

**特性：**
- 默认权重平均分配（简单场景）
- 用户可手动调整权重和优先级（高级场景）
- 自动健康检查和故障检测
- 无感知的高可用性

### 为什么这个方案会成功

1. **极简主义设计**：只做四件事，但做到极致，避免功能臃肿
2. **用户无感哲学**：复杂性被隐藏，用户只需关心"用哪个模型"
3. **轻量级部署**：Docker 单容器，开箱即用，无复杂依赖
4. **双配置模式**：Web 界面（新手友好）+ 配置文件（Git 友好）
5. **开源社区**：降低使用门槛，赋能整个开发者生态

### 高层次愿景

**短期（MVP）：** 为个人开发者提供可靠的 AI 基础设施工具

**中期（6-12个月）：** 成为个人开发者首选的轻量级 AI 网关解决方案

**长期（1-2年）：** 构建开源生态，推动"优雅的 AI 基础设施"理念

---

## Target Users

### Primary User Segment: AI 应用独立开发者

**人物画像：Alex**

**基本信息：**
- 年龄：25-40 岁
- 职业：独立开发者、自由职业者、创业者
- 技术背景：熟悉基本的后端开发和 API 集成
- 项目类型：个人 SaaS 产品、AI 工具、自动化脚本

**当前行为与工作流：**
- 使用 3-5 个不同的 AI 供应商（OpenAI、Claude、DeepSeek 等）
- 通过 new-api 或直接集成管理多个 API Key
- 使用多种工具：ChatGPT Web（日常对话）、Claude Code（编程）、Cursor（代码补全）
- 需要在成本和可靠性之间平衡

**痛点：**
- 每个供应商的模型命名规则不同，配置管理混乱
- 不同工具需要不同 API 格式，需要单独配置
- 供应商限流或宕机时需要手动切换，影响开发效率
- new-api 功能过于复杂，维护成本高

**目标：**
- 快速搭建可靠的 AI 接入层，专注于业务开发
- 降低 AI 服务成本（通过多供应商优化）
- 提高服务可靠性（避免单点故障）
- 简化配置和运维工作

**为什么会使用 Siriusx-API：**
- Docker 一键部署，5 分钟即可启动
- 统一的模型名称管理，降低认知负担
- 自动故障转移，提高可靠性
- 支持 Claude Code 等专业工具的原生格式
- 开源免费，配置文件可版本控制

**使用场景：**

**场景 1：统一管理多个供应商**
```
Alex 有 3 个供应商的 Claude API Key
→ 在 Siriusx-API 中聚合为 claude-sonnet-4
→ 设置权重：A(50%) + B(30%) + C(20%)
→ 所有应用只需配置一个统一名称
```

**场景 2：工具格式适配**
```
Alex 的工具箱：
- ChatGPT Next Web → 使用 /v1/chat/completions
- Claude Code → 使用 /v1/messages
→ 同一个 claude-sonnet-4 模型，配置两个端点
→ 不同工具各取所需
```

**场景 3：无感故障转移**
```
供应商 A 突然限流
→ Siriusx-API 自动检测
→ 请求路由到 B 或 C
→ Alex 的应用完全不受影响
```

### Secondary User Segment: 小型创业团队

**团队规模：** 2-5 人

**团队特点：**
- 技术驱动的初创团队
- 预算有限，需要成本优化
- 需要快速迭代，不想在基础设施上花费过多时间
- 希望配置可共享、可版本控制

**需求差异：**
- 可能需要团队协作的配置管理
- 需要基本的访问日志和监控
- 希望配置文件可以通过 Git 管理和同步

**Siriusx-API 的价值：**
- 配置文件模式天然支持 Git 协作
- 轻量级部署，降低运维成本
- 统一的 AI 接入层，减少团队内部配置差异

---

## Goals & Success Metrics

### Business Objectives

- **用户采用率**：MVP 上线 3 个月内达到 100 个活跃用户（GitHub Stars 作为代理指标）
- **社区参与度**：获得至少 10 个外部贡献者的 Pull Request
- **部署成功率**：用户首次部署成功率 > 90%（通过用户反馈和 Issue 追踪）
- **开源生态**：成为 GitHub 上"AI 网关"类别的前 10 项目（按 Stars 排名）

### User Success Metrics

- **配置时间**：用户从 Docker 启动到第一个 API 调用成功，平均时间 < 10 分钟
- **故障转移效果**：供应商故障时，95% 的请求能够成功转移到备用供应商
- **用户满意度**：通过社区反馈，获得 > 80% 的正面评价
- **留存率**：用户 30 天留存率 > 60%（通过 Docker Hub 下载趋势和社区活跃度）

### Key Performance Indicators (KPIs)

- **部署量**：Docker Hub 总下载量 > 1000 次（3 个月内）
- **API 吞吐量**：单实例支持 > 100 QPS（满足个人开发者需求）
- **系统可用性**：Siriusx-API 自身的可用性 > 99.5%
- **社区增长**：GitHub Star 增长率 > 20 个/月
- **文档完整性**：文档覆盖 100% 核心功能，新手引导成功率 > 85%

---

## MVP Scope

### Core Features (Must Have)

#### 1. **供应商管理**
- **功能描述**：添加、编辑、删除供应商配置（URL + API Key）
- **自动发现**：调用供应商的 `/v1/models` 接口，自动获取可用模型列表
- **健康检查**：定期检测供应商可用性（心跳机制）
- **配置持久化**：支持配置文件（YAML）和 Web 界面双模式
- **必须原因**：这是整个系统的基础，没有供应商管理就无法提供服务

#### 2. **模型聚合与映射**
- **统一命名**：用户定义标准化的模型名称（如 `claude-sonnet-4`）
- **细粒度映射**：精确到"供应商 A 的 claude-4"级别
- **权重配置**：为每个供应商-模型组合设置权重（默认平均分配）
- **优先级设置**：定义故障转移的优先顺序
- **必须原因**：这是 Siriusx-API 的核心差异化特性，解决命名混乱问题

#### 3. **格式转换引擎**
- **OpenAI 格式输出**：默认端点 `/v1/chat/completions`，保持 OpenAI 兼容
- **Claude 格式输出**：自定义端点 `/v1/messages`，支持 Claude Code
- **请求/响应转换**：自动在 OpenAI ↔ Claude 格式之间转换
- **流式响应支持**：支持 SSE（Server-Sent Events）流式输出
- **必须原因**：格式自由切换是核心价值主张之一，必须在 MVP 实现

#### 4. **负载均衡与故障转移**
- **权重路由**：根据配置的权重分配请求到不同供应商
- **自动故障检测**：请求失败时（超时、5xx、限流）自动检测
- **智能转移**：按优先级自动切换到备用供应商
- **重试策略**：支持可配置的重试次数和退避策略
- **必须原因**：这是"用户无感"体验的核心保障，必须在 MVP 实现

#### 5. **Web 管理界面（基础版）**
- **供应商管理页面**：添加/编辑/删除供应商
- **模型配置页面**：创建统一模型，配置映射关系和权重
- **状态监控面板**：显示供应商健康状态、请求统计（基础）
- **配置导入/导出**：支持从配置文件导入，导出为配置文件
- **必须原因**：降低使用门槛，让非技术用户也能快速上手

#### 6. **配置文件支持**
- **YAML 格式**：清晰、易读的配置文件格式
- **热重载**（可选）：配置文件变更时自动重载（如果时间允许）
- **Git 友好**：配置文件可版本控制和协作
- **必须原因**：满足高级用户需求，支持 DevOps 工作流

#### 7. **Docker 容器化**
- **单容器部署**：所有组件打包在一个 Docker 镜像中
- **配置挂载**：支持 Volume 挂载配置文件
- **环境变量配置**：支持通过环境变量覆盖关键配置
- **健康检查**：容器健康检查端点
- **必须原因**：轻量级部署是核心定位，必须在 MVP 实现

### Out of Scope for MVP

为了保持 MVP 的极简和聚焦，以下功能**不在 MVP 范围内**：

- ❌ **用户系统与权限管理**：个人使用，暂不需要多用户
- ❌ **额度管理与计费**：过于复杂，与"轻量级"定位冲突
- ❌ **高级统计分析**：基础监控即可，详细分析可后续添加
- ❌ **令牌管理**：暂不需要 API Token 认证
- ❌ **智能模型推荐**：自动推荐聚合方案，可作为 Phase 2 功能
- ❌ **动态权重优化**：根据性能自动调整权重，过于复杂
- ❌ **多租户支持**：暂不考虑 SaaS 场景
- ❌ **Codex 格式支持**：优先支持 OpenAI 和 Claude，其他格式后续添加
- ❌ **高级缓存机制**：简单场景不需要，可后续优化
- ❌ **Webhook 通知**：故障通知等功能可后续添加

### MVP Success Criteria

MVP 被认为成功，当满足以下条件：

1. **功能完整性**：
   - 7 个核心功能全部实现并通过测试
   - 文档覆盖所有核心功能

2. **用户体验**：
   - 用户能在 10 分钟内完成首次部署和配置
   - 用户能成功配置至少 2 个供应商和 1 个聚合模型
   - 故障转移功能正常工作（通过测试验证）

3. **技术指标**：
   - 单实例支持 > 50 QPS（满足个人开发者基本需求）
   - 请求延迟 < 200ms（不含上游 AI 服务延迟）
   - Docker 镜像大小 < 100MB（轻量级）

4. **社区反馈**：
   - 至少 3 个早期用户成功部署并提供正面反馈
   - 没有阻断性 Bug（P0 级别问题）

---

## Post-MVP Vision

### Phase 2 Features

**预计时间：** MVP 后 1-2 个迭代（2-3 个月）

#### 1. 监控与可观测性增强
- 请求日志系统（可选持久化）
- 性能指标收集（响应时间、成功率等）
- 供应商状态历史记录
- 简单的监控仪表板（图表展示）

#### 2. 配置迁移工具
- 从 new-api 导入配置
- 从其他 AI 网关导入配置
- 配置模板库（常见供应商预配置）

#### 3. 高级负载策略
- 基于响应时间的动态权重调整
- 成本优先路由（优先使用低成本供应商）
- 时间段策略（如高峰期分流）

#### 4. 更多格式支持
- Codex 格式（Cursor 等工具）
- Gemini 格式
- 自定义格式转换规则

### Long-term Vision (1-2 Years)

**愿景：** 成为个人开发者和小团队的标准 AI 基础设施组件

**关键方向：**

1. **插件生态系统**
   - 支持自定义中间件和拦截器
   - 社区贡献的格式转换插件
   - 第三方监控和日志集成

2. **多协议支持**
   - GraphQL API
   - gRPC 支持
   - WebSocket 长连接

3. **边缘部署优化**
   - 更小的镜像（< 50MB）
   - ARM 架构支持
   - 边缘函数部署（Cloudflare Workers、Vercel Edge）

4. **智能化能力**
   - 智能模型推荐（基于名称相似度）
   - 自适应故障恢复策略
   - 异常检测和预警

### Expansion Opportunities

**水平扩展：**
- 支持更多 AI 服务类型（图像生成、语音识别等）
- 支持传统 API 的聚合和负载均衡（通用网关能力）

**垂直扩展：**
- 企业版（多租户、高级权限管理）
- SaaS 服务（托管版 Siriusx-API）

**生态合作：**
- 与 AI 工具（Claude Code、Cursor 等）官方集成
- 与云服务商合作（一键部署到云平台）

---

## Technical Considerations

### Platform Requirements

- **Target Platforms:** Linux、macOS、Windows（通过 Docker）
- **Container Runtime:** Docker 20.10+ 或兼容的容器运行时（Podman）
- **Minimum Resources:**
  - CPU: 1 核
  - 内存: 512MB
  - 磁盘: 500MB（镜像 + 配置 + 日志）
- **Performance Requirements:**
  - 单实例支持 100+ QPS
  - P95 延迟 < 200ms（不含上游服务）
  - 启动时间 < 5 秒

### Technology Preferences

#### **Backend:** Go (推荐)

**理由：**
- 与 new-api 技术栈一致，便于参考和集成
- 高性能、低内存占用（符合轻量级定位）
- 优秀的并发支持（Goroutine）
- 编译为单一二进制文件，便于 Docker 打包
- 丰富的 HTTP/网络库生态

**替代方案：** Node.js (如果团队更熟悉)

#### **Frontend:** Vue 3 + Element Plus (推荐)

**理由：**
- 轻量级（与项目定位一致）
- 组件库成熟（Element Plus）
- 学习曲线平缓
- 适合快速开发管理后台

**替代方案：** React + Ant Design

#### **Database:** SQLite (MVP)

**理由：**
- 无需单独部署数据库服务（符合轻量级定位）
- 配置文件即数据库，易于备份和迁移
- 满足个人开发者场景的性能需求

**未来考虑：** PostgreSQL（如果需要扩展到企业场景）

#### **Hosting/Infrastructure:** Docker + Docker Compose

**理由：**
- 用户自托管为主（符合个人开发者需求）
- Docker 一键部署
- 未来可扩展到 Kubernetes（如有需要）

### Architecture Considerations

#### **Repository Structure**
```
siriusx-api/
├── cmd/                  # 主程序入口
├── internal/             # 内部包
│   ├── provider/         # 供应商管理
│   ├── model/            # 模型聚合
│   ├── transformer/      # 格式转换
│   ├── balancer/         # 负载均衡
│   ├── config/           # 配置管理
│   └── api/              # API 路由
├── web/                  # 前端代码
├── config/               # 配置文件示例
├── docs/                 # 文档
├── Dockerfile
├── docker-compose.yml
└── README.md
```

#### **Service Architecture**
- **单体架构**（Monolithic）：所有功能在一个进程中
- **模块化设计**：清晰的模块边界，便于测试和维护
- **无状态服务**：配置存储在 SQLite 或配置文件，服务本身无状态

#### **Integration Requirements**
- **上游集成**：基于 new-api 的 OpenAI 兼容接口（作为输入）
- **下游集成**：支持多个 AI 供应商的 API（OpenAI、Anthropic 等）
- **标准协议**：HTTP/HTTPS、REST API、SSE（流式响应）

#### **Security/Compliance**
- **API Key 加密存储**：敏感信息加密存储在数据库中
- **HTTPS 支持**：支持 TLS 证书配置
- **Rate Limiting**：防止滥用（基础限流）
- **审计日志**：记录关键操作（配置变更、供应商切换等）
- **无外部数据传输**：所有数据在用户本地处理，不上传到外部服务

---

## Constraints & Assumptions

### Constraints

#### **Budget**
- **开发预算**：个人项目，无资金预算（纯开源）
- **运营预算**：用户自托管，无运营成本
- **时间预算**：兼职开发，预计 2-3 个月完成 MVP

#### **Timeline**
- **Phase 1（需求与设计）**：1 周（2025-09-30 至 2025-10-06）
- **Phase 2（MVP 开发）**：6-8 周（2025-10-07 至 2025-11-30）
- **Phase 3（测试与文档）**：2 周（2025-12-01 至 2025-12-15）
- **Phase 4（发布与推广）**：1 周（2025-12-16 至 2025-12-22）

**里程碑：**
- 2025-10-06：PRD 和架构设计完成
- 2025-11-15：MVP 核心功能开发完成
- 2025-12-15：测试和文档完成，进入 Beta 测试
- 2025-12-22：正式发布 v1.0

#### **Resources**
- **开发人员**：1 人（全栈开发）
- **测试用户**：3-5 人（早期用户和朋友）
- **技术支持**：开源社区（GitHub Issues、Discussions）

#### **Technical**
- 必须保持轻量级（Docker 镜像 < 100MB）
- 必须支持 Docker 部署（跨平台兼容）
- 必须与 new-api 输入格式兼容（降低迁移成本）
- 必须支持至少 OpenAI 和 Claude 两种格式输出

### Key Assumptions

- **假设 1**：目标用户熟悉基本的 Docker 操作
- **假设 2**：用户已经有至少 2 个 AI 供应商的 API Key
- **假设 3**：用户主要使用 OpenAI 和 Claude 模型（其他模型需求较低）
- **假设 4**：个人开发者更关注"可用性"而非"功能丰富度"
- **假设 5**：配置文件模式对高级用户有吸引力（Git 友好）
- **假设 6**：开源社区会对"轻量级 AI 网关"有需求和兴趣
- **假设 7**：Go 语言生态能满足所有技术需求（HTTP、并发、性能等）
- **假设 8**：SQLite 能满足个人开发者的性能需求（QPS < 1000）

---

## Risks & Open Questions

### Key Risks

#### **风险 1：用户采用率低于预期**
- **描述**：开源项目竞争激烈，可能无法吸引足够用户
- **影响**：项目无法形成社区，长期维护动力不足
- **缓解策略**：
  - 重点突出差异化特性（模型聚合 + 格式转换）
  - 提供详细文档和示例
  - 在相关社区（Reddit、V2EX、Hacker News）推广
  - 制作演示视频和教程

#### **风险 2：技术复杂度超出预期**
- **描述**：格式转换（OpenAI ↔ Claude）可能比预想的复杂
- **影响**：开发时间延长，MVP 延期
- **缓解策略**：
  - 优先实现最常用的模型和格式
  - 参考 new-api 和 claudecoderouter 的实现
  - 逐步支持更多格式，不强求 MVP 全覆盖

#### **风险 3：性能不达标**
- **描述**：请求转发和格式转换增加延迟，用户体验下降
- **影响**：用户放弃使用，转向其他方案
- **缓解策略**：
  - 早期进行性能基准测试
  - 优化关键路径（使用缓存、连接池等）
  - 如果单体架构性能不足，考虑优化或重构

#### **风险 4：供应商 API 变更**
- **描述**：AI 供应商可能变更 API 格式或限制
- **影响**：Siriusx-API 功能失效，需要紧急修复
- **缓解策略**：
  - 设计灵活的格式转换层，便于适配变更
  - 监控供应商 API 变更公告
  - 建立快速响应机制（Hotfix 流程）

#### **风险 5：开源许可证冲突**
- **描述**：依赖的开源库可能有不兼容的许可证
- **影响**：法律风险，需要更换依赖或调整许可证
- **缓解策略**：
  - 使用宽松许可证（MIT 或 Apache 2.0）
  - 审查所有依赖的许可证
  - 避免使用 GPL 等强 Copyleft 许可证的库

### Open Questions

- **Q1**：是否需要支持流式响应（SSE）？
  - **重要性**：高（许多 AI 应用需要流式输出）
  - **决策点**：MVP 阶段是否必须实现

- **Q2**：如何处理不同供应商的 rate limit 差异？
  - **重要性**：中（影响负载均衡策略）
  - **决策点**：是否需要在配置中显式声明每个供应商的限流规则

- **Q3**：配置文件变更时，是否需要热重载？
  - **重要性**：中（影响用户体验）
  - **决策点**：热重载实现复杂度 vs 用户便利性

- **Q4**：是否需要支持自定义请求/响应拦截器（中间件）？
  - **重要性**：中（扩展性考虑）
  - **决策点**：MVP 是否需要，还是 Phase 2 功能

- **Q5**：多租户场景是否在考虑范围内？
  - **重要性**：低（当前定位个人开发者）
  - **决策点**：是否预留架构扩展空间

- **Q6**：如何平衡"功能完整性"和"极简主义"？
  - **重要性**：高（核心设计哲学）
  - **决策点**：每个新功能请求都需要严格评估是否符合"极简"定位

### Areas Needing Further Research

- **OpenAI ↔ Claude 格式转换细节**
  - 研究两者 API 的完整对应关系
  - 确认所有参数能否完整映射
  - 识别不兼容的功能和参数

- **健康检查策略**
  - 如何高效检测供应商状态而不增加额外成本
  - 心跳频率如何设置
  - 如何判断"故障"（超时？5xx？限流？）

- **配置文件格式设计**
  - YAML 结构如何设计才能平衡可读性和灵活性
  - 是否需要 JSON Schema 验证
  - 如何处理配置迁移（版本升级）

- **竞品深度分析**
  - new-api：详细了解架构和限制
  - LiteLLM：分析其模型聚合方式
  - One API：评估其优缺点

- **Go vs Node.js 技术选型**
  - 性能对比（并发、内存占用）
  - 开发效率对比
  - 生态成熟度对比

---

## Appendices

### A. Research Summary

#### 头脑风暴会议成果（2025-09-30）

**使用技巧：**
- 角色扮演（个人开发者视角）
- 五个为什么（深挖核心价值）
- 假设颠覆（差异化特性探索）

**核心洞察：**
1. **智能负载均衡 + 无感故障转移**：用户完全无需感知供应商切换
2. **灵活的双端点格式转换**：同一模型可提供 OpenAI 和 Claude 两种格式
3. **精细化供应商与模型管理**：细粒度到"供应商-模型"级别的控制

**设计原则：**
- 极简主义：只做四件事，但做到极致
- 用户无感：把复杂性隐藏，展现优雅
- 灵活性优先：双配置模式，适配不同习惯
- 轻量部署：Docker 开箱即用

**详细文档：** `docs/brainstorming-session-results.md`

#### 竞品分析（初步）

**new-api：**
- 优势：功能全面、生态成熟
- 劣势：模型命名统一困难、Claude Code 支持麻烦、过于臃肿
- 启示：Siriusx-API 应避免功能臃肿，专注核心价值

**LiteLLM：**
- 优势：支持多种模型格式转换
- 劣势：模型名称管理不够灵活
- 启示：模型聚合是 Siriusx-API 的差异化优势

**One API：**
- 优势：开源、活跃社区
- 劣势：架构较重，部署复杂
- 启示：轻量级部署是 Siriusx-API 的核心竞争力

### B. Stakeholder Input

**早期用户反馈（非正式）：**
- 用户 A："new-api 功能太多了，我只想要一个简单的负载均衡工具"
- 用户 B："Claude Code 需要原生格式，现有方案都不够优雅"
- 用户 C："希望配置能放在 Git 里管理，Web 界面虽然方便但不适合版本控制"

### C. References

- **new-api GitHub**：https://github.com/Calcium-Ion/new-api
- **Claude API 文档**：https://docs.anthropic.com/claude/reference
- **OpenAI API 文档**：https://platform.openai.com/docs/api-reference
- **LiteLLM GitHub**：https://github.com/BerriAI/litellm
- **Docker 最佳实践**：https://docs.docker.com/develop/dev-best-practices/

---

## Next Steps

### Immediate Actions

1. **技术选型确认**
   - 完成 Go vs Node.js 的最终决策
   - 确认前端框架（Vue 3 vs React）
   - 验证 SQLite 性能是否满足需求

2. **深度技术调研**
   - 研究 OpenAI ↔ Claude 格式的完整映射
   - 分析 new-api 和 claudecoderouter 的源码
   - 确定健康检查和故障转移的技术方案

3. **架构设计**
   - 绘制详细的系统架构图
   - 设计数据模型（供应商、模型、配置等）
   - 设计 API 接口规范
   - 设计配置文件格式（YAML Schema）

4. **PRD 编写**
   - 转换到 PM 角色，创建详细的产品需求文档
   - 定义每个功能的详细需求和验收标准
   - 设计用户流程和交互原型

5. **项目初始化**
   - 创建 GitHub Repository
   - 初始化项目结构
   - 配置开发环境和 CI/CD

### PM Handoff

This Project Brief provides the full context for **Siriusx-API**.

**交接给 PM（Product Manager - John）：**

下一步请 PM 基于这份项目简报创建详细的 PRD（Product Requirements Document），重点包括：

1. **功能详细规格**：每个核心功能的详细需求和验收标准
2. **用户流程设计**：用户从安装到使用的完整旅程
3. **技术规格说明**：API 接口、数据模型、配置文件格式
4. **非功能需求**：性能、安全、可扩展性要求
5. **Epic 和 Story 拆分**：为开发阶段准备可执行的任务

**建议：** 使用 BMad 工作流，切换到 PM 角色（`*agent pm`），然后执行 `*create-prd` 命令。

---

*文档版本：v1.0*
*最后更新：2025-09-30*
*状态：Draft - 待 PM 审阅*
